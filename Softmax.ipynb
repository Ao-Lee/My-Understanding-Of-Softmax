{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a k-classs classification problem(suppose k>=2), for any given input $x$, the probability of y is given by:\n",
    "$$\n",
    "p(y=i|x;\\theta )\\quad =\\quad \\frac { { e }^{ { s }_{ i } } }{ \\sum _{ k }^{  }{ { e }^{ { s }_{ k } } }  } \n",
    "$$\n",
    "where\n",
    "$$\n",
    "{ S }_{ i }={ \\theta  }_{ i }^{ T }x\n",
    "$$\n",
    "and for convinience, we denote $p(y=i|x;\\theta )$ as ${ p }_{ i }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, in a 4-class classification problem, a label for a given input $x$ could be y=[0,1,0,0]. This assumption is true for most of the cases, and the loss function (for only one input) under this assumption is:\n",
    "$$\n",
    "-\\log { { p }_{ { y }_{ i } } } \n",
    "$$\n",
    "and the total loss of one batch is an average over this fomula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general form is that: the labels can also be a distribution, rather than discrete values. For example: y=[0.2, 0.2, 0.4, 0.2] And the general form of loss function is(suppose there are k classes):\n",
    "$$\n",
    "-\\sum _{ k=1 }^{ K }{ { y }_{ k }\\log { { p }_{ k } }  } \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for binary cases (k=2), the loss fomula can be reduced to:\n",
    "$$\n",
    "-y\\log { (p) } -(1-y)\\log { (1-p) } \n",
    "$$\n",
    "\n",
    "This is quite common if the output of the network is a pixel value of a binary image. Suppose the ground truth of the pixel value is 25, we devide the value by 255 to squzee it to a range between 0 and 1, then transorm the label to a probability dirstribution [0.1, 0.9] for that pixel. \n",
    "\n",
    "In tensorflow and keras, if we set the loss of our network to be 'binary_crossentropy', this is the loss actually computed, and this is also why binary_crossentropy accept fraction labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let q = pos_weight, the loss function parameteried by q is given:\n",
    "$$\n",
    "-qy\\log { (p) } -(1-y)\\log { (1-p) }\n",
    "$$\n",
    "A value pos_weights > 1 decreases the false negative count, hence increasing the recall. Conversely setting pos_weights < 1 decreases the false positive count and increases the precision. \n",
    "For more details: see https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work\n",
    "https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "https://github.com/keras-team/keras/issues/3653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
